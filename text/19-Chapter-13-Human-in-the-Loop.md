# 第 13 章：人机协同

人机协同（Human-in-the-Loop，HITL）模式代表了智能体（Agents）开发和部署中的一项关键策略。它刻意地将人类认知的独特优势（如判断力、创造力和细致入微的理解力）与人工智能（AI）的计算能力和效率交织在一起。这种战略性整合不仅仅是一种选择，而且往往是一种必需，尤其是当 AI 系统日益深入地嵌入到关键决策过程中时。

HITL 的核心原则是确保 AI 在道德边界内运行，遵守安全协议，并以最佳效率实现其目标。这些担忧在具有复杂性、模糊性或重大风险的领域尤为突出，因为在这些领域，AI 错误或误解可能带来严重后果。在这种情况下，完全自主（即 AI 系统在没有任何人工干预的情况下独立运行）可能被证明是不明智的。HITL 承认这一现实，并强调即使 AI 技术日新月异，人类的监督、战略性输入和协作互动仍然不可或缺。

HITL 方法从根本上围绕着人工智能与人类智能之间的协同作用展开。HITL 并不将 AI 视为人类工作者的替代品，而是将其定位为一种增强和提升人类能力的工具。这种增强可以采取多种形式，从自动化常规任务到提供数据驱动的洞察以辅助人类决策。最终目标是创建一个协作生态系统，使人类和 AI 智能体都能利用各自的独特优势，实现任何一方都无法单独取得的成果。

在实践中，HITL 可以通过多种方式实现。一种常见的方法是让人类担任验证者或审查者，检查 AI 的输出以确保准确性并识别潜在错误。另一种实现方式是让人类主动引导 AI 的行为，实时提供反馈或进行纠正。在更复杂的设置中，人类可以作为合作伙伴与 AI 协作，通过交互式对话或共享界面共同解决问题或制定决策。无论具体实现方式如何，HITL 模式都强调了保持人类控制和监督的重要性，确保 AI 系统始终与人类的伦理、价值观、目标和社会期望保持一致。
## Human-in-the-Loop Pattern Overview | 人机协同模式概述

人机协同（HITL）模式将人工智能与人类输入相结合，以增强智能体（Agent）的能力。这种方法承认，最佳的 AI 性能往往需要自动化处理与人类洞察力的结合，尤其是在具有高度复杂性或伦理考量的场景中。HITL 并非旨在取代人类输入，而是通过确保关键判断和决策基于人类理解，从而增强人类的能力。

HITL 包含几个关键方面：人类监督（Human Oversight），涉及监控 AI 智能体的性能和输出（例如，通过日志审查或实时仪表板），以确保其遵守准则并防止不良后果。当 AI 智能体遇到错误或模糊场景时，会发生干预与纠正（Intervention and Correction），此时智能体可能请求人类干预；人类操作员可以纠正错误、提供缺失数据或指导智能体，这也有助于智能体未来的改进。用于学习的人类反馈（Human Feedback for Learning）被收集并用于优化 AI 模型，这在诸如“基于人类反馈的强化学习” (RLHF) 等方法中尤为突出，其中人类偏好直接影响智能体的学习轨迹。决策增强（Decision Augmentation）是指 AI 智能体向人类提供分析和建议，由人类做出最终决定，通过 AI 生成的洞察力来增强人类决策，而非完全自主。人机协作（Human-Agent Collaboration）是一种合作互动，人类和 AI 智能体各自贡献其优势；常规数据处理可能由智能体负责，而创造性问题解决或复杂谈判则由人类管理。最后，上报策略（Escalation Policies）是既定的协议，规定了智能体应在何时以及如何将任务上报给人类操作员，以防止在超出智能体能力的情况下出错。

实施 HITL 模式使得智能体（Agents）能够在不允许或不可行完全自主的敏感行业中得到应用。它还通过反馈循环提供了一种持续改进的机制。例如，在金融领域，大额企业贷款的最终批准需要人类信贷员来评估领导者品格等定性因素。同样，在法律领域，正义和问责的核心原则要求人类法官对量刑等涉及复杂道德推理的关键决策保留最终决定权。

<strong>注意事项：</strong>尽管 HITL 模式有其好处，但它也有明显的局限性，其中最主要的是缺乏可扩展性。虽然人类监督能提供高准确性，但操作员无法管理数百万个任务，这在根本上形成了一种权衡，因此往往需要一种混合方法，将自动化用于规模化，将 HITL 用于保证准确性。此外，该模式的有效性在很大程度上取决于人类操作员的专业知识；例如，虽然 AI 可以生成软件代码，但只有熟练的开发人员才能准确识别细微错误并提供正确的指导来修复它们。这种对专业知识的需求也适用于使用 HITL 生成训练数据时，因为人类标注者可能需要接受专门培训，以学习如何纠正 AI 才能产生高质量的数据。最后，实施 HITL 会引发严重的隐私问题，因为敏感信息在暴露给人类操作员之前通常必须经过严格的匿名化处理，这又增加了一层流程复杂性。
## Practical Applications & Use Cases | 实际应用与用例

人机协同模式在众多行业和应用中都至关重要，特别是在那些对准确性、安全性、伦理或细致入微的理解有极高要求的领域。

- <strong>内容审核：</strong>AI 智能体可以快速过滤海量在线内容，筛查违规行为（例如，仇恨言论、垃圾信息）。然而，模棱两可或处于边缘情况的内容会被上报给人类审核员进行审查并做出最终决定，以确保细致入微的判断并遵守复杂的政策。

- <strong>自动驾驶：</strong>虽然自动驾驶汽车能自主处理大多数驾驶任务，但它们被设计为在 AI 无法自信应对的复杂、不可预测或危险情况下（例如，极端天气、异常路况）将控制权交还给人类驾驶员。

- <strong>金融欺诈检测：</strong>AI 系统可以根据模式标记可疑交易。但是，高风险或模棱两可的警报通常会发送给人类分析师，由他们进行深入调查、联系客户，并最终确定交易是否具有欺诈性。

- <strong>法律文件审查：</strong>AI 可以快速扫描和分类数千份法律文件，以识别相关条款或证据。然后，人类法律专业人士会审查 AI 的发现，核实其准确性、上下文和法律含义，特别是对于关键案件。

- <strong>客户支持（复杂查询）：</strong>聊天机器人可以处理常规的客户咨询。如果用户的问题过于复杂、情绪激动，或者需要 AI 无法提供的情感共鸣，对话将被无缝转接给人类支持专员。

- <strong>数据标注：</strong>AI 模型通常需要大量标注数据进行训练。人类被引入循环，以准确标注图像、文本或音频，为 AI 提供学习所需的“地面实况”（ground truth）。随着模型的发展，这是一个持续的过程。

- <strong>生成式 AI 优化：</strong>当大语言模型（LLM）生成创意内容（例如，营销文案、设计理念）时，人类编辑或设计师会审查和优化输出，确保其符合品牌准则，能与目标受众产生共鸣，并保持高质量。

- <strong>自治网络：</strong>AI 系统能够利用关键性能指标（KPI）和已识别的模式来分析警报、预测网络问题和流量异常。然而，关键决策（例如处理高风险警报）经常被上报给人类分析师。这些分析师会进行进一步调查，并最终决定是否批准网络变更。

该模式例证了一种实用的 AI 实施方法。它利用 AI 来提高可扩展性和效率，同时保持人类监督，以确保质量、安全和伦理合规性。

「Human-on-the-loop」（人类在环）是此模式的一种变体，其中人类专家定义总体策略，然后由 AI 处理即时操作以确保合规性。我们来看两个例子：

  <strong>自动化金融交易系统：</strong>在此场景中，人类金融专家设定总体投资策略和规则。例如，人类可能定义策略为：「保持 70% 科技股和 30% 债券的投资组合，对任何单一公司的投资不超过 5%，并在任何股票价格低于其购买价 10% 时自动卖出。」然后，AI 实时监控股票市场，在这些预定义条件满足时立即执行交易。AI 正在基于人类操作员设定的较慢、更具战略性的策略来处理即时、高速的行动。

  <strong>现代化呼叫中心：</strong>在此设置中，人类经理为客户互动建立高级策略。例如，经理可能设定规则，如「任何提到『服务中断』的电话应立即转接给技术支持专员」，或者「如果客户的语气表明高度沮丧，系统应主动提出将他们直接转接给人工坐席。」然后，AI 系统处理初始的客户互动，实时倾听并解释他们的需求。它通过即时转接电话或提供上报选项来自主执行经理的策略，而无需为每个个案都进行人工干预。这使得 AI 能够根据人类操作员提供的较慢、战略性的指导来管理大量的即时行动。
## Hands-On Code Example | 实战代码示例

为了演示人机协同模式，ADK 智能体（agent）可以识别需要人工审查的场景并发起上报流程。这允许在智能体的自主决策能力有限或需要复杂判断的情况下进行人工干预。这不是一个孤立的功能；其他流行的框架也采用了类似的功能。例如，LangChain 也提供了实现此类交互的工具。

# Placeholder for tools (replace with actual implementations if needed) 

   # This would typically transfer to a human queue in a real system

   # Get customer info from state
      
        # Add as a system message before the first content

此代码提供了使用 Google 的 ADK 创建技术支持智能体的蓝图，该智能体围绕 HITL 框架设计。该智能体作为智能的第一道防线，配置了具体指令，并配备了像 <code>troubleshoot_issue</code>、<code>create_ticket</code> 和 <code>escalate_to_human</code> 这样的工具来管理完整的支持工作流。上报工具是 HITL 设计的核心部分，确保复杂或敏感的案例被转交给人类专家。

该架构的一个关键特性是通过专用回调函数实现的深度个性化能力。在联系大语言模型（LLM）之前，该函数会从智能体的状态中动态检索客户特定数据，例如他们的姓名、等级和购买历史。然后，此上下文作为系统消息被注入到提示中，使智能体能够提供高度定制化和信息充分的、并引用用户历史的回复。通过将结构化工作流与必要的人类监督和动态个性化相结合，此代码提供了一个 ADK 如何促进开发复杂且强大的 AI 支持解决方案的实践范例。
## At a Glance | 要点速览

<strong>问题所在：</strong>AI 系统，包括先进的大语言模型（LLM），通常难以处理需要细致入微的判断、伦理推理或对复杂模糊上下文有深刻理解的任务。在事关重大的环境中部署完全自主的 AI 会带来巨大风险，因为错误可能导致严重的安全、财务或伦理后果。这些系统缺乏人类所拥有的内在创造力和常识性推理能力。因此，在关键决策过程中完全依赖自动化通常是不明智的，并且可能损害系统的整体有效性和可信度。

<strong>解决之道：</strong>人机协同（HITL）模式通过将人类监督战略性地整合到 AI 工作流中，提供了一种标准化的解决方案。这种智能体式（agentic）方法创建了一种共生伙伴关系，其中 AI 处理计算密集型任务和数据处理，而人类则提供关键的验证、反馈和干预。通过这样做，HITL 确保 AI 的行动与人类价值观和安全协议保持一致。这种协作框架不仅降低了完全自动化的风险，而且通过不断从人类输入中学习来增强系统的能力。最终，这会带来更稳健、准确和合乎伦理的结果，这是人类或 AI 任何一方都无法单独实现的。

<strong>经验法则：</strong>当在医疗、金融或自动驾驶系统等错误会带来严重安全、伦理或财务后果的领域部署 AI 时，请使用此模式。对于涉及大语言模型（LLM）无法可靠处理的模糊性和细微差别的任务（如内容审核或复杂的客户支持上报），该模式至关重要。当目标是利用高质量、人工标注的数据持续改进 AI 模型，或优化生成式 AI 的输出以满足特定质量标准时，也应采用 HITL。
**Visual summary** | **可视化总结：**

![Human in the loop design pattern](/images/chapter13_fig1.png)

<strong>图 1：</strong>人机协同设计模式
## Key Takeaways | 核心要点

核心要点包括：

- 人机协同（HITL）将人类的智能和判断力整合到 AI 工作流中。

- 它对于复杂或高风险场景下的安全性、伦理性和有效性至关重要。

- 关键方面包括人类监督、干预、用于学习的反馈以及决策增强。

- 上报策略对于智能体了解何时应转交给人类至关重要。

- HITL 允许负责任的 AI 部署和持续改进。

- 人机协同的主要缺点是其固有的可扩展性不足（在准确性和处理量之间造成了权衡）以及它对高技能领域专家进行有效干预的依赖。

- 其实施带来了运营上的挑战，包括需要培训人类操作员以生成数据，以及需要通过匿名化敏感信息来解决隐私问题。
## Conclusion | 结语

本章探讨了至关重要的人机协同（HITL）模式，强调了其在创建稳健、安全和合乎伦理的 AI 系统中的作用。我们讨论了将人类监督、干预和反馈整合到智能体工作流中如何能显著提高其性能和可信度，尤其是在复杂和敏感的领域。实际应用展示了 HITL 的广泛效用，从内容审核、医疗诊断到自动驾驶和客户支持。概念性代码示例让我们得以一窥 ADK 如何通过上报机制促进这些人机交互。随着 AI 能力的不断进步，HITL 仍然是负责任 AI 发展的基石，确保人类价值观和专业知识始终处于智能系统设计的核心。
## References | 参考文献

1. 《机器学习中的人机协同综述》，Xingjiao Wu, Luwei Xiao, Yixuan Sun, Junhang Zhang, Tianlong Ma, Liang He, [https://arxiv.org/abs/2108.00941](https://arxiv.org/abs/2108.00941)
