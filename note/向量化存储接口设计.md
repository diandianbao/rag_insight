# 向量化存储接口设计

## 设计目标

基于现有的RAG项目架构，设计一套完整的向量化存储接口，实现文档片段向量化并存储到pgvector数据库中。

## 接口架构设计

### 1. 核心数据模型

#### DocumentChunk (扩展)
```python
@dataclass
class DocumentChunk:
    content: str
    metadata: dict
    chunk_id: str
    document_id: str
    vector: Optional[List[float]] = None  # 新增：向量数据
    embedding_model: Optional[str] = None  # 新增：使用的embedding模型
    created_at: Optional[datetime] = None  # 新增：创建时间
```

#### VectorStoreConfig
```python
@dataclass
class VectorStoreConfig:
    database_url: str
    table_name: str = "document_chunks"
    embedding_model: str = "qwen3-embedding:4b"
    embedding_endpoint: str = "http://localhost:11434/api/embeddings"
    vector_dimension: int = 1024  # qwen3-embedding:4b的向量维度
    batch_size: int = 100
```

### 2. 核心接口定义

#### VectorStore 抽象基类
```python
from abc import ABC, abstractmethod

class VectorStore(ABC):
    """向量存储抽象基类"""

    @abstractmethod
    def connect(self) -> bool:
        """连接向量数据库"""
        pass

    @abstractmethod
    def create_table(self) -> bool:
        """创建向量存储表"""
        pass

    @abstractmethod
    def embed_text(self, text: str) -> List[float]:
        """文本向量化"""
        pass

    @abstractmethod
    def store_chunks(self, chunks: List[DocumentChunk]) -> bool:
        """存储文档块向量"""
        pass

    @abstractmethod
    def search_similar(self, query: str, top_k: int = 5) -> List[DocumentChunk]:
        """相似度搜索"""
        pass

    @abstractmethod
    def batch_embed_and_store(self, chunks: List[DocumentChunk]) -> bool:
        """批量向量化并存储"""
        pass
```

#### PgVectorStore 具体实现类
```python
class PgVectorStore(VectorStore):
    """基于pgvector的向量存储实现"""

    def __init__(self, config: VectorStoreConfig):
        self.config = config
        self.connection = None

    def connect(self) -> bool:
        """连接PostgreSQL + pgvector数据库"""
        pass

    def create_table(self) -> bool:
        """创建支持向量的文档块表"""
        pass

    def embed_text(self, text: str) -> List[float]:
        """使用Ollama的embedding模型向量化文本"""
        pass

    def store_chunks(self, chunks: List[DocumentChunk]) -> bool:
        """存储文档块向量到数据库"""
        pass

    def search_similar(self, query: str, top_k: int = 5) -> List[DocumentChunk]:
        """基于向量相似度搜索相关文档"""
        pass

    def batch_embed_and_store(self, chunks: List[DocumentChunk]) -> bool:
        """批量处理：向量化 + 存储"""
        pass
```

### 3. 数据库表结构设计

#### document_chunks 表
```sql
CREATE TABLE IF NOT EXISTS document_chunks (
    id SERIAL PRIMARY KEY,
    chunk_id VARCHAR(255) UNIQUE NOT NULL,
    document_id VARCHAR(255) NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB,
    vector VECTOR(1024),  -- pgvector扩展类型
    embedding_model VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- 索引优化
    INDEX idx_document_id (document_id),
    INDEX idx_created_at (created_at),
    INDEX idx_vector USING ivfflat (vector)  -- 向量索引
);
```

### 4. 接口方法详细设计

#### 4.1 连接管理
- `connect()`: 建立数据库连接，验证pgvector扩展
- `disconnect()`: 关闭数据库连接
- `health_check()`: 检查数据库和模型服务状态

#### 4.2 向量化服务
- `embed_text(text)`: 单文本向量化
- `embed_batch(texts)`: 批量文本向量化
- `get_embedding_model_info()`: 获取模型信息

#### 4.3 存储操作
- `store_single(chunk)`: 存储单个文档块
- `store_batch(chunks)`: 批量存储文档块
- `update_chunk(chunk_id, new_content)`: 更新文档块
- `delete_chunk(chunk_id)`: 删除文档块

#### 4.4 检索操作
- `search_similar(query, top_k)`: 相似度搜索
- `search_by_document(document_id)`: 按文档ID搜索
- `hybrid_search(query, keywords, top_k)`: 混合搜索（向量+关键词）

#### 4.5 管理操作
- `get_statistics()`: 获取存储统计信息
- `cleanup_old_data(days)`: 清理旧数据
- `backup_database()`: 数据库备份

### 5. 错误处理设计

#### 自定义异常类
```python
class VectorStoreError(Exception):
    """向量存储基础异常"""
    pass

class EmbeddingError(VectorStoreError):
    """向量化异常"""
    pass

class DatabaseError(VectorStoreError):
    """数据库操作异常"""
    pass

class ConnectionError(VectorStoreError):
    """连接异常"""
    pass
```

### 6. 配置管理

#### 环境变量配置
```python
import os

class Config:
    DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://localhost/hello_vector")
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "qwen3-embedding:4b")
    EMBEDDING_ENDPOINT = os.getenv("EMBEDDING_ENDPOINT", "http://localhost:11434/api/embeddings")
    VECTOR_DIMENSION = int(os.getenv("VECTOR_DIMENSION", "1024"))
    BATCH_SIZE = int(os.getenv("BATCH_SIZE", "100"))
```

### 7. 集成点设计

#### 与现有模块的集成
1. **DocumentSplitter**: 接收分割后的文档块
2. **KeywordExtractor**: 可选的关键词增强检索
3. **Reranker**: 检索结果的精排

#### 使用流程
```python
# 1. 文档分割
document_chunks = document_splitter.split_document(document_content)

# 2. 向量化存储
vector_store = PgVectorStore(config)
vector_store.connect()
vector_store.create_table()
vector_store.batch_embed_and_store(document_chunks)

# 3. 检索
query = "智能体设计模式"
results = vector_store.search_similar(query, top_k=10)

# 4. 重排序（可选）
reranked_results = reranker.rerank(query, results)
```

### 8. 性能考虑

1. **批量处理**: 支持批量向量化和存储
2. **连接池**: 数据库连接池管理
3. **缓存**: 频繁查询结果的缓存
4. **索引**: 向量索引和元数据索引优化
5. **异步处理**: 支持异步向量化操作

### 9. 扩展性设计

1. **多模型支持**: 可切换不同的embedding模型
2. **多数据库支持**: 支持ChromaDB、Pinecone等
3. **插件架构**: 支持自定义向量化插件
4. **监控集成**: 集成Prometheus监控指标

这个接口设计为后续的向量化存储实现提供了清晰的架构指导，确保了与现有RAG系统的无缝集成。